{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d7b70b",
   "metadata": {},
   "source": [
    "## Without web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70738589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a430e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af82d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"How up to date is your training knowledge?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053dbeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: I don’t have live internet access by default, and my training data covers information up to roughly late 2023 (with some topics having small updates earlier in 2024). I may not know about events, publications, or developments that happened after that unless you provide details or I’m using a browsing tool.\n",
      "\n",
      "If you need the very latest info, you can:\n",
      "- enable a browse/tool mode (if available here) and I’ll fetch current sources, or\n",
      "- tell me the topic and date range and I’ll work from what I know and flag any potential gaps.\n",
      "\n",
      "If you have a specific topic in mind, I can tailor my answer and note where post-cutoff updates might apply.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e906d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='How up to date is your training knowledge?', additional_kwargs={}, response_metadata={}, id='1c8c2010-7273-419d-bbd7-3fa0cd6a358a'),\n",
      "              AIMessage(content='Short answer: I don’t have live internet access by default, and my training data covers information up to roughly late 2023 (with some topics having small updates earlier in 2024). I may not know about events, publications, or developments that happened after that unless you provide details or I’m using a browsing tool.\\n\\nIf you need the very latest info, you can:\\n- enable a browse/tool mode (if available here) and I’ll fetch current sources, or\\n- tell me the topic and date range and I’ll work from what I know and flag any potential gaps.\\n\\nIf you have a specific topic in mind, I can tailor my answer and note where post-cutoff updates might apply.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1304, 'prompt_tokens': 15, 'total_tokens': 1319, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cxc5Gog0PY1YktMZFriQtZglG5ED4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bb84f-8f98-7901-81d1-f37e2c173035-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 15, 'output_tokens': 1304, 'total_tokens': 1319, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1152}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e24ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My training data goes up to June 2024. I don’t have real-time internet access by default, so I may not know about events or developments that happened after that date. If your platform has browsing enabled, I can look up current information; otherwise, I can help interpret or analyze information you provide and explain concepts up to that cutoff. If you need the latest specifics, tell me the topic and I’ll suggest how to verify it with current sources.\n"
     ]
    }
   ],
   "source": [
    "# Example to compare the answer Agent answer with chat model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat_model = init_chat_model(\n",
    "    model=\"gpt-5-nano\")\n",
    "\n",
    "response = chat_model.invoke([\"How up to date is your training knowledge\"])\n",
    "                             \n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1230fd7",
   "metadata": {},
   "source": [
    "## My question to CoPilot\n",
    "Why does the agentic answer in cell 3 and 4 say that the model was trained with info up to late 2023 and the chat model answer in later cells says June 2024?\n",
    "\n",
    "## Answer\n",
    "The discrepancy you're seeing is because the two models (used by the agent and the chat model) have different knowledge cutoffs based on when their training data ends. This is common with different model versions or configurations:\n",
    "\n",
    "The agentic answer (from the model used in the agent setup) reports a cutoff of \"late 2023\" (with some updates into early 2024).\n",
    "The chat model answer reports a cutoff of \"June 2024\".\n",
    "Even though the notebook code specifies \"gpt-5-nano\" for both, in practice, this could resolve to different underlying models or versions depending on your environment/configuration (e.g., one might be an older GPT-4 variant, while the other is a newer GPT-4o or similar). Model providers often release updates with extended cutoffs, and different instances or API calls might use different versions.\n",
    "\n",
    "If this is intentional for the demo, it highlights how agents can use tools to access real-time data beyond their base training cutoff, while direct model queries are limited to their trained knowledge. If you want consistent cutoffs, ensure both use the exact same model version in your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c49546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model name: gpt-5-nano\n",
      "Chat model details: profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x78df982035c0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78df98203290> root_client=<openai.OpenAI object at 0x78df982011c0> root_async_client=<openai.AsyncOpenAI object at 0x78df982035f0> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n",
      "Graph nodes: ['__start__', 'model', '__end__']\n",
      "Graph edges: [Edge(source='__start__', target='model', data=None, conditional=False), Edge(source='model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "Node: __start__\n",
      "Node object: Node(id='__start__', name='__start__', data=RunnableCallable(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None)\n",
      "\n",
      "Node: model\n",
      "Node object: Node(id='model', name='model', data=model_node(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None)\n",
      "\n",
      "Node: __end__\n",
      "Node object: Node(id='__end__', name='__end__', data=None, metadata=None)\n",
      "Agent config: {'recursion_limit': 10000, 'configurable': {}}\n",
      "Agent attributes: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__ror__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aprepare_state_snapshot', '_atransform_stream_with_config', '_batch_with_config', '_call_with_config', '_defaults', '_migrate_checkpoint', '_prepare_state_snapshot', '_repr_mimebundle_', '_transform_stream_with_config', 'abatch', 'abatch_as_completed', 'abulk_update_state', 'aclear_cache', 'aget_graph', 'aget_state', 'aget_state_history', 'aget_subgraphs', 'ainvoke', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'attach_branch', 'attach_edge', 'attach_node', 'aupdate_state', 'batch', 'batch_as_completed', 'bind', 'builder', 'bulk_update_state', 'cache', 'cache_policy', 'channels', 'checkpointer', 'clear_cache', 'config', 'config_schema', 'config_specs', 'context_schema', 'copy', 'debug', 'get_config_jsonschema', 'get_context_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'get_state', 'get_state_history', 'get_subgraphs', 'input_channels', 'input_schema', 'interrupt_after_nodes', 'interrupt_before_nodes', 'invoke', 'map', 'name', 'nodes', 'output_channels', 'output_schema', 'pick', 'pipe', 'retry_policy', 'schema_to_mapper', 'step_timeout', 'store', 'stream', 'stream_channels', 'stream_channels_asis', 'stream_channels_list', 'stream_eager', 'stream_mode', 'transform', 'trigger_to_nodes', 'update_state', 'validate', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"
     ]
    }
   ],
   "source": [
    "# Underlying model of the chat\n",
    "print(\"Chat model name:\", chat_model.model_name)\n",
    "print(\"Chat model details:\", chat_model)\n",
    "\n",
    "# Underlying model of the agent in the graph structure\n",
    "graph = agent.get_graph()\n",
    "print(\"Graph nodes:\", list(graph.nodes.keys()))\n",
    "print(\"Graph edges:\", list(graph.edges))\n",
    "\n",
    "# Inspect Individual Nodes\n",
    "model_name_in_graph = None\n",
    "for node_name, node in graph.nodes.items():\n",
    "    print(f\"\\nNode: {node_name}\")\n",
    "    if hasattr(node, '__dict__'):\n",
    "        pprint(vars(node))  # Print all attributes of the node\n",
    "    else:\n",
    "        print(f\"Node object: {node}\")\n",
    "    if hasattr(node, 'runnable'):\n",
    "        print(f\"Runnable: {node.runnable}\")\n",
    "        if hasattr(node.runnable, 'model_name'):\n",
    "            print(f\"Model name: {node.runnable.model_name}\")\n",
    "            if node_name == 'model':\n",
    "                model_name_in_graph = node.runnable.model_name\n",
    "\n",
    "# Since the agent was created with create_agent, it should have stored config\n",
    "print(\"Agent config:\", getattr(agent, 'config', 'No config attribute'))\n",
    "print(\"Agent attributes:\", dir(agent))\n",
    "\n",
    "# If the model name is still not showing in the Mermaid diagram, it means the underlying model\n",
    "# in the agent is not directly accessible through these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bebb0",
   "metadata": {},
   "source": [
    "## Add web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)\n",
    "\n",
    "web_search.invoke(\"Who is the current mayor of San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537120ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[web_search]\n",
    ")\n",
    "\n",
    "question = HumanMessage(content=\"Who is the current mayor of San Francisco?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd33a79",
   "metadata": {},
   "source": [
    "trace: https://smith.langchain.com/public/59432173-0dd6-49e8-9964-b16be6048426/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54303bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
