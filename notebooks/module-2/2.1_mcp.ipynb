{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a helpful assistant that answers user questions about LangChain, LangGraph and LangSmith.\n",
      "\n",
      "    You can use the following tools/resources to answer user questions:\n",
      "    - search_web: Search the web for information\n",
      "    - github_file: Access the langchain-ai repo files\n",
      "\n",
      "    If the user asks a question that is not related to LangChain, LangGraph or LangSmith, you should say \"I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"\n",
      "\n",
      "    You may try multiple tool and resource calls to answer the user's question.\n",
      "\n",
      "    You may also ask clarifying questions to the user to better understand their question.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='4cc4155f-daae-4ae0-a27d-92756802baa7'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 271, 'total_tokens': 363, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQ4tF3eBQZAUOVTRrkIG5JGqFyud', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc3c3-f821-7b63-9dd5-5c21cc204ccb-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_4w0FTABn8qdh5EzIpwhdQD3j', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 271, 'output_tokens': 92, 'total_tokens': 363, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-tools/\",\\n      \"title\": \"langchain-mcp-tools\",\\n      \"content\": \"LangChain\\'s official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\",\\n      \"score\": 0.9999918,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.9999831,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two\",\\n      \"score\": 0.99997354,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca\",\\n      \"title\": \"The Complete Guide to langchain-mcp-adapters\",\\n      \"content\": \"This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"] }, \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\\\\\"claude-sonnet-4-5-20250929\\\\\", tools)# Use the agentresponse = await agent.ainvoke({ \\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\\\\\"openai:gpt-4.1\\\\\")client = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"./examples/math_server.py\\\\\"], \\\\\"transport\\\\\": \\\\\"stdio\\\\\" }, \\\\\"weather\\\\\": { \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\\\\\"messages\\\\\"]) return {\\\\\"messages\\\\\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \\\\\"call_model\\\\\")builder.add_conditional_edges(\\\\\"call_model\\\\\", tools_condition)builder.add_edge(\\\\\"tools\\\\\", \\\\\"call_model\\\\\").\",\\n      \"score\": 0.9999577,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.81,\\n  \"request_id\": \"46a558bc-6518-4ff7-80bf-61df87a2da88\"\\n}', 'id': 'lc_be87c815-09e4-4c62-a512-632035d925b9'}], name='search_web', id='efea59bb-a356-49a1-b3f5-25cbb5ec6e1a', tool_call_id='call_4w0FTABn8qdh5EzIpwhdQD3j', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999956, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-tools/', 'title': 'langchain-mcp-tools', 'content': \"LangChain's official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\", 'score': 0.9999918, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.9999831, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two', 'score': 0.99997354, 'raw_content': None}, {'url': 'https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca', 'title': 'The Complete Guide to langchain-mcp-adapters', 'content': 'This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \"math\": { \"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/math_server.py\"] }, \"weather\": { \"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)# Use the agentresponse = await agent.ainvoke({ \"messages\": [{\"role\": \"user\", \"content\": \"what\\'s (3 + 5) x 12?\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\"openai:gpt-4.1\")client = MultiServerMCPClient({ \"math\": { \"command\": \"python\", \"args\": [\"./examples/math_server.py\"], \"transport\": \"stdio\" }, \"weather\": { \"url\": \"http://localhost:8000/mcp\", \"transport\": \"streamable_http\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\"messages\"]) return {\"messages\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \"call_model\")builder.add_conditional_edges(\"call_model\", tools_condition)builder.add_edge(\"tools\", \"call_model\").', 'score': 0.9999577, 'raw_content': None}], 'response_time': 0.81, 'request_id': '46a558bc-6518-4ff7-80bf-61df87a2da88'}}}),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1309, 'prompt_tokens': 1613, 'total_tokens': 2922, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1280, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQ50OA8bhLEfihIWWplCGS73k6AZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc3c4-0eff-7781-a8ad-8db2afce90ef-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain mcp adapters python example'}, 'id': 'call_dpwldBYvLM5LUX2PTAxONgLQ', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1613, 'output_tokens': 1309, 'total_tokens': 2922, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1280}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain mcp adapters python example\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.85421735,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` ¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.82186085,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca\",\\n      \"title\": \"The Complete Guide to langchain-mcp-adapters - Medium\",\\n      \"content\": \"This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"] }, \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\\\\\"claude-sonnet-4-5-20250929\\\\\", tools)# Use the agentresponse = await agent.ainvoke({ \\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\\\\\"openai:gpt-4.1\\\\\")client = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"./examples/math_server.py\\\\\"], \\\\\"transport\\\\\": \\\\\"stdio\\\\\" }, \\\\\"weather\\\\\": { \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\\\\\"messages\\\\\"]) return {\\\\\"messages\\\\\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \\\\\"call_model\\\\\")builder.add_conditional_edges(\\\\\"call_model\\\\\", tools_condition)builder.add_edge(\\\\\"tools\\\\\", \\\\\"call_model\\\\\").\",\\n      \"score\": 0.8058924,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to MCP Adapters\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\\\\\"http://localhost:3000/mcp\\\\\", transport_type=\\\\\"sse\\\\\", headers={ \\\\\"Authorization\\\\\": f\\\\\"Bearer {os.getenv(\\'API_TOKEN\\')}\\\\\", \\\\\"User-Agent\\\\\": \\\\\"LangChain-MCP-Client/1.0\\\\\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Create a new lead for John Smith with email [email\\xa0protected]\\\\\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.\",\\n      \"score\": 0.7827459,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \\\\\"user_id\\\\\": user_id}  args ={**request.args, \\\\\"user_id\\\\\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\\\\\"gpt-4o\\\\\", tools, context_schema=Context) agent = create_agent(\\\\\"gpt-4o\\\\\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, context={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"}  context ={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"})).\",\\n      \"score\": 0.7724109,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.83,\\n  \"request_id\": \"d73e4f00-5645-4759-a428-69bc160786fb\"\\n}', 'id': 'lc_24809d25-6fe4-44bf-8761-49d8ec25f2f6'}], name='search_web', id='8710b091-9324-473d-9b7a-ab40a2f085ae', tool_call_id='call_dpwldBYvLM5LUX2PTAxONgLQ', artifact={'structured_content': {'result': {'query': 'langchain mcp adapters python example', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.85421735, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` ¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.82186085, 'raw_content': None}, {'url': 'https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca', 'title': 'The Complete Guide to langchain-mcp-adapters - Medium', 'content': 'This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \"math\": { \"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/math_server.py\"] }, \"weather\": { \"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)# Use the agentresponse = await agent.ainvoke({ \"messages\": [{\"role\": \"user\", \"content\": \"what\\'s (3 + 5) x 12?\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\"openai:gpt-4.1\")client = MultiServerMCPClient({ \"math\": { \"command\": \"python\", \"args\": [\"./examples/math_server.py\"], \"transport\": \"stdio\" }, \"weather\": { \"url\": \"http://localhost:8000/mcp\", \"transport\": \"streamable_http\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\"messages\"]) return {\"messages\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \"call_model\")builder.add_conditional_edges(\"call_model\", tools_condition)builder.add_edge(\"tools\", \"call_model\").', 'score': 0.8058924, 'raw_content': None}, {'url': 'https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters', 'title': 'LangChain MCP Integration: Complete Guide to MCP Adapters', 'content': 'from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\"python\", \"database_mcp_server.py\"], transport_type=\"stdio\", environment={ \"DATABASE_URL\": \"postgresql://user:pass@localhost:5432/mydb\", \"MAX_CONNECTIONS\": \"10\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\"gpt-4\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \"input\": \"Find all customers who made purchases over $500 in the last month\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\"http://localhost:3000/mcp\", transport_type=\"sse\", headers={ \"Authorization\": f\"Bearer {os.getenv(\\'API_TOKEN\\')}\", \"User-Agent\": \"LangChain-MCP-Client/1.0\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \"input\": \"Create a new lead for John Smith with email [email\\xa0protected]\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.', 'score': 0.7827459, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \"\"\"Inject user credentials into MCP tool calls.\"\"\" \"\"\"Inject user credentials into MCP tool calls.\"\"\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \"user_id\": user_id}  args ={**request.args, \"user_id\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\"gpt-4o\", tools, context_schema=Context) agent = create_agent(\"gpt-4o\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}  context ={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"})).', 'score': 0.7724109, 'raw_content': None}], 'response_time': 0.83, 'request_id': 'd73e4f00-5645-4759-a428-69bc160786fb'}}}),\n",
      "              AIMessage(content='Here’s a concise overview of the langchain-mcp-adapters library and what it’s for.\\n\\nWhat it is\\n- A library (Python) that converts Model Context Protocol (MCP) tools into LangChain- and LangGraph-compatible tools.\\n- It lets you interact with MCP tool servers from LangChain/LangGraph agents and load tools from multiple MCP servers at once.\\n- It’s designed to make it easy to tap into the large ecosystem of MCP tool servers without writing custom adapters for each one.\\n\\nWhy you’d use it\\n- You can connect to hundreds of MCP servers and load their tools as LangChain tools, prompts, or resources.\\n- Agents can pull tools from multiple MCP servers simultaneously, enabling more powerful tool combinations.\\n- It integrates with LangGraph workflows (e.g., ToolNode, StateGraph) and supports common MCP server setups (including authentication, timeouts, and error handling).\\n\\nKey components and concepts\\n- MultiServerMCPClient: The main client to connect to multiple MCP servers, load tools, and fetch resources.\\n- load_mcp_tools / load_mcp_resources: Helpers to bring MCP tools and resources into the LangChain/LangGraph environment.\\n- Tool execution interception: Interceptors/Callbacks to customize how tool calls are handled.\\n- Server options: Supports various transports (stdio, HTTP/streamable HTTP, etc.), per-server or global timeouts, optional tool name prefixes, and optional OAuth2 authentication for secure MCP servers.\\n- Language bindings: Python (LangChain/LangGraph integration). There are also JavaScript/TypeScript resources in the ecosystem (see the npm page for mcp-adapters).\\n\\nWhere to learn more (official references)\\n- GitHub: langchain-ai/langchain-mcp-adapters (primary repo with code and examples)\\n- LangChain docs (Python MCP adapters): reference.langchain.com/python/langchain_mcp_adapters/\\n- LangChain changelog announcing MCP Adapters for LangChain and LangGraph\\n- PyPI pages and related guides:\\n  - Python ecosystem discussions and installation notes (search for langchain-mcp-adapters in PyPI/LangChain docs)\\n  - JavaScript/TypeScript side: @langchain/mcp-adapters (NPM) for the JS/TS ecosystem\\n\\n Quick example (high level)\\n- Connect to multiple MCP servers:\\n  - Create a MultiServerMCPClient with server configurations (e.g., math via stdio, weather via HTTP/HTTPS).\\n- Load tools:\\n  - Use client.get_tools() to fetch all available MCP tools from the connected servers.\\n- Create an agent and invoke:\\n  - Create a LangChain/LangGraph agent with the loaded tools.\\n  - Invoke the agent with a user message; the agent can call MCP tools as needed.\\n\\nExample snippet (illustrative; adjust paths/configs to your setup)\\n- Python\\n  - from langchain_mcp_adapters.client import MultiServerMCPClient\\n  - from langchain.agents import create_agent\\n  - from langchain.chat_models import init_chat_model\\n  - client = MultiServerMCPClient({\\n      \"math\": {\"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"./examples/math_server.py\"]},\\n      \"weather\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\"}\\n    })\\n  - tools = await client.get_tools()\\n  - llm = init_chat_model(\"openai:gpt-4.1\")\\n  - agent = create_agent(\"gpt-4-connector\", tools)\\n  - response = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in NYC?\"}]})\\n\\nNotes and variants\\n- There is also a JavaScript/TypeScript path for MCP adapters: @langchain/mcp-adapters (NPM) if you’re working in the JS/TS ecosystem.\\n- The MCP adapters are designed to simplify integration with MCP servers, including handling authentication, timeouts, and multi-server composition.\\n- Documentation and examples show how to set up tool loading, server connections, and integrating with LangGraph workflows.\\n\\nWould you like:\\n- A minimal, runnable Python example with a concrete MCP server (or a mock MCP server) to copy-paste?\\n- Details on installing and configuring the Python library for a specific MCP setup (e.g., with OAuth, timeouts, or tool prefixes)?\\n- A quick comparison of the Python MCP adapters versus the JavaScript/TypeScript variant?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2589, 'prompt_tokens': 4264, 'total_tokens': 6853, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1664, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQ5GNDZdCZRwPHXy5UF35V87VZmr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3c4-534d-75d0-b6a5-d57de5415b04-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 4264, 'output_tokens': 2589, 'total_tokens': 6853, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1664}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fca732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the weather in San Francisco?\", additional_kwargs={}, response_metadata={}, id='391ca575-2d68-46db-a5a2-b42651884fac'),\n",
      "              AIMessage(content=\"I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 347, 'prompt_tokens': 267, 'total_tokens': 614, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQ80I9YUkTnLGxHqgpbyBRj1zPH3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3c6-e9da-71f0-a32d-2ad6835c6323-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 267, 'output_tokens': 347, 'total_tokens': 614, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}})]}\n"
     ]
    }
   ],
   "source": [
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to install uv (pip install uv), checked with which uvx (/home/codespace/.python/current/bin/uvx)\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='d7a0ede8-a213-4434-964c-4cafbd2d10c9'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 296, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQBrnYztIjjrFSd3MLzyOJTA5yGA', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc3ca-90a8-7a12-8aa5-5aa4d0c268b4-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_Pg8OVrgfYnvdIAt2ufeUBxq7', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 296, 'output_tokens': 219, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-15T17:33:14-05:00\",\\n  \"day_of_week\": \"Thursday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_b0371c48-af9b-46ac-ba08-1cd1f8cded0b'}], name='get_current_time', id='10056142-4bd3-4b17-8610-54eb1616bb27', tool_call_id='call_Pg8OVrgfYnvdIAt2ufeUBxq7'),\n",
      "              AIMessage(content='It’s 5:33 PM on Thursday, January 15, 2026 in New York (Eastern Standard Time, UTC-5). Want me to convert this to another time zone?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 432, 'prompt_tokens': 379, 'total_tokens': 811, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQBw3wV0O13Xcrlp7gzZf1outGU3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3ca-9f78-72c1-b44b-50b82d3115d2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 379, 'output_tokens': 432, 'total_tokens': 811, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
